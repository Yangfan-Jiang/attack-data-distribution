{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e309cd-148d-45a9-b14d-0909e6ba7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLModel import *\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import joblib\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm, rv_histogram, entropy, wasserstein_distance\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807af16d-7f81-4ed8-87d8-64f20e1f1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_client = 4\n",
    "x=[]\n",
    "y=[]\n",
    "data_pth = 'abalone/'\n",
    "feature = 'length'\n",
    "output_class = 5\n",
    "\n",
    "def local_data(data_pth, feature, kid):\n",
    "    # test set\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(1, num_client+1):\n",
    "        x.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/test_x\"+str(i)+\".npy\"))\n",
    "        y.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/test_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    non_iid = []\n",
    "    real_x = []\n",
    "    real_y = []\n",
    "    for i in range(1, num_client+1):\n",
    "        real_x.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/train_x\"+str(i)+\".npy\"))\n",
    "        real_y.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/train_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    global_x = np.concatenate((x[0], x[1]))\n",
    "    global_x = np.concatenate((global_x, x[2]))\n",
    "    global_x = np.concatenate((global_x, x[3]))\n",
    "\n",
    "    global_y = np.concatenate((y[0], y[1]))\n",
    "    global_y = np.concatenate((global_y, y[2]))\n",
    "    global_y = np.concatenate((global_y, y[3]))\n",
    "    #print(global_x.shape)\n",
    "    #print(global_y.shape)\n",
    "    num_feature = global_x.shape[1]\n",
    "    \n",
    "    dist_info = []\n",
    "    discrete_feature = []\n",
    "    for i in range(num_feature):\n",
    "        # discrete?\n",
    "        unique_feature = np.unique(global_x[:, i], return_counts=True)\n",
    "        if len(unique_feature[0]) < 25:\n",
    "            # compute probability (frequence) of each element\n",
    "            dist_info.append(tuple((unique_feature[0], unique_feature[1]/unique_feature[1].sum())))\n",
    "            discrete_feature.append(i)\n",
    "        else:\n",
    "            # compute mean and std\n",
    "            mu = np.mean(global_x[:, i])\n",
    "            sigma = np.std(global_x[:, i])\n",
    "            dist_info.append(tuple((mu, sigma)))\n",
    "        \n",
    "    return real_x, real_y, global_x, global_y, dist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e047b4d-1fc0-42e0-808d-cba6e203eeae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_dpsgd(eps, i, kid):\n",
    "    global_iter = 5\n",
    "    w_ml = []\n",
    "        \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    param_name = list(m0.state_dict().keys())\n",
    "        \n",
    "    mg = model(num_feature, output_class).to(device)\n",
    "    mg.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    mg.eval()\n",
    "    mg_params = torch.cat([torch.flatten(mg.state_dict()[name]) for name in param_name], -1)\n",
    "            \n",
    "    for client_id in range(num_client):\n",
    "        model_par = []\n",
    "\n",
    "        # compute weights\n",
    "        ml_params = []\n",
    "        ml = []\n",
    "\n",
    "        delta_m_local = []\n",
    "        w_list = []\n",
    "\n",
    "        for epoch in range(1,global_iter+1):\n",
    "            # load local models\n",
    "            m0 = model(num_feature, output_class).to(device)\n",
    "            m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch'+str(epoch)+'/eps'+str(eps)+'/'+str(client_id)+'.pth', \n",
    "                                           map_location=torch.device('cpu')))\n",
    "            m0.eval()\n",
    "            m0_params = torch.cat([torch.flatten(m0.state_dict()[name]) for name in param_name], -1)\n",
    "            ml.append(copy.deepcopy(m0.state_dict()))\n",
    "            \n",
    "            delta_m_local = torch.norm(m0_params - mg_params, p=2)\n",
    "            w_list.append(delta_m_local.item())\n",
    "            \n",
    "        w_list = torch.softmax(torch.Tensor(w_list), dim=0)\n",
    "\n",
    "        new_par = copy.deepcopy(m0.state_dict())\n",
    "        for name in new_par:\n",
    "            new_par[name] = torch.zeros(new_par[name].shape)\n",
    "        for idx, par in enumerate(ml):\n",
    "            for name in new_par:\n",
    "                new_par[name] += par[name]*w_list[idx]\n",
    "        \n",
    "        m0 = model(num_feature, output_class).to(device)\n",
    "        m0.load_state_dict(copy.deepcopy(new_par))\n",
    "        m0.eval()\n",
    "        w_ml.append(copy.deepcopy(m0))\n",
    "\n",
    "    g = model(num_feature, output_class).to(device)\n",
    "    g.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    g.eval()\n",
    "    \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    m0.eval()\n",
    "    return (w_ml, g, m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bd936-b4c8-4dd0-a6be-3f30904f8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack(c, feature_dis, num_feature, ml, g, lr, var_factor):\n",
    "    r_min = 0\n",
    "    r_max = 1\n",
    "    x = torch.tensor(np.random.uniform(r_min, r_max, num_feature)).float().requires_grad_()\n",
    "    opt = optim.Adam([x], lr=lr)\n",
    "\n",
    "    for _ in range(25):\n",
    "        yg = torch.softmax(g(x),0)\n",
    "        yc = torch.softmax(ml[c](x),0)\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(yg, yc) + var_factor*x.var()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # projection\n",
    "        x = torch.clamp(x, r_min, r_max).clone().detach().numpy()\n",
    "        for f in range(num_feature):\n",
    "            # discrete data\n",
    "            if type(feature_dis[f][0]) is np.ndarray:\n",
    "                x[f] = np.random.choice(feature_dis[f][0], 1)[0]\n",
    "        x = torch.tensor(x).requires_grad_()\n",
    "        opt = optim.Adam([x], lr=lr)\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def gen_synthesis_PGD(dist_info, num_feature, num_client, fake_sample_size, ml, g, lr, var_factor):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    for c in range(num_client):\n",
    "        tmpsample = []\n",
    "        for i in range(fake_sample_size):\n",
    "            x = PGD_attack(c, dist_info, num_feature, ml, g, lr, var_factor)\n",
    "            tmpsample.append(x)\n",
    "            if len(tmpsample)%400 == 0:\n",
    "                print(len(tmpsample))\n",
    "        sample_list.append(copy.deepcopy(np.array(tmpsample)))\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def PGD_noised_sample(feature_dis, num_feature, num_client, fake_sample_size, ml, g, m0, lr, var_factor):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    sample_sim_list = []\n",
    "    sample_noise_lsit = []\n",
    "    sample_simnoise_lise = []\n",
    "    subsample_list = []\n",
    "    \n",
    "    for c in range(num_client):\n",
    "        pgd_sample = []\n",
    "        pgd_noise = []\n",
    "\n",
    "        d_max = -1\n",
    "        d_min = 9999\n",
    "\n",
    "        cnt = 0\n",
    "        d_list = []\n",
    "        d_threshold  = 0\n",
    "        \n",
    "        while len(pgd_noise) < fake_sample_size:\n",
    "            x = PGD_attack(c, feature_dis, num_feature, ml, g, lr, var_factor)\n",
    "            if len(pgd_sample) < fake_sample_size:\n",
    "                pgd_sample.append(copy.deepcopy(x))\n",
    "                pgd_noise.append(copy.deepcopy(x))\n",
    "\n",
    "        sample_list.append(copy.deepcopy(np.array(pgd_sample)))\n",
    "        sample_noise_lsit.append(copy.deepcopy(np.array(pgd_noise)))\n",
    "    return (sample_list, sample_noise_lsit)\n",
    "\n",
    "\n",
    "\n",
    "def hc_noise_attack(feature_dis, num_feature, num_client, fake_sample_size, ml, g, prior_dist):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    subsample_list = []\n",
    "    \n",
    "    for c in range(num_client):\n",
    "        hc_sample = []\n",
    "        hc_subsample = []\n",
    "        threshold = 1e-3\n",
    "        \n",
    "        d_max = -1\n",
    "        d_min = 9999\n",
    "        \n",
    "        cnt = 0\n",
    "        d_list = []\n",
    "        d_threshold  = 0\n",
    "        while len(hc_subsample) < fake_sample_size:\n",
    "            x = hc_attack(c, feature_dis, num_feature, ml, g, prior_dist, threshold)\n",
    "            j = 0\n",
    "            \n",
    "            # generate a valid sample\n",
    "            while x is None:\n",
    "                x = hc_attack(c, feature_dis, num_feature, ml, g, prior_dist, threshold)\n",
    "                j += 1\n",
    "                if j > 10:\n",
    "                    threshold *= 2\n",
    "                    #print(\"threshold =\", threshold)\n",
    "                    j = 0\n",
    "                    \n",
    "            if len(hc_sample) < fake_sample_size:\n",
    "                hc_sample.append(copy.deepcopy(x))\n",
    "                hc_subsample.append(copy.deepcopy(x))\n",
    "            \n",
    "        sample_list.append(copy.deepcopy(np.array(hc_sample)))\n",
    "        subsample_list.append(copy.deepcopy(np.array(hc_subsample)))\n",
    "    return (sample_list, subsample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8172b-702c-4496-aa8a-1d21a0f2e80b",
   "metadata": {},
   "source": [
    "## Attack LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655e415-ebe3-487a-872f-324e0f9c8307",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "model = LogisticRegression\n",
    "# model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59199b-f13a-47de-9189-ca54b4877c6d",
   "metadata": {},
   "source": [
    "## Attack Black-Box LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a2810-e67b-495f-a8d0-f318fb90b5e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'hc'\n",
    "model = LogisticRegression\n",
    "# model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b16ea5-b900-4e33-90cf-a8df5036086e",
   "metadata": {},
   "source": [
    "## Attack NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a82a4b-c145-4aa2-9264-d626bef30aa9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "# model = LogisticRegression\n",
    "model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b40fd8-ca95-4885-921e-e0baacb50de4",
   "metadata": {},
   "source": [
    "### Varying dropout $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c590b1b-4b9d-4cdd-b09b-81bde12be12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "\n",
    "eps = 4\n",
    "\n",
    "for model in [LightMLP, LightMLP1, LightMLP3, LightMLP4, LightMLP5]:\n",
    "    if hasattr(model, '__name__'):\n",
    "        model_name = model.__name__ \n",
    "    print(model_name)\n",
    "    \n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "            \n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "            \n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e946af-b1bc-45e8-a8ec-6f9f51f76a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55482eb2-0f1f-49bd-aadd-e8edc971c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_label_attack(client_num, sample, ml, g):\n",
    "    predit_label = [[] for _ in range(client_num)]\n",
    "    predit_prob = [[] for _ in range(client_num)]\n",
    "    # random_guess = [[np.random.choice(list(range(output_class))) for _ in range(1250)] for _ in range(client_num)]\n",
    "    random_guess = []\n",
    "    for c in range(client_num):\n",
    "        for x in sample[c]:\n",
    "            pred_g = torch.softmax(ml[c](torch.tensor(x).float()),0)\n",
    "                \n",
    "            pred_label = torch.max(pred_g, 0)[1].item()\n",
    "            predit_label[c].append(pred_label)\n",
    "            for i in range(output_class):\n",
    "                for _ in range(100):\n",
    "                    if np.random.random() < pred_g[i]:\n",
    "                        predit_prob[c].append(i)\n",
    "                    \n",
    "        for i in range(output_class):\n",
    "            predit_label[c].append(i)\n",
    "            predit_prob[c].append(i)\n",
    "            \n",
    "    # generate grandom guess results\n",
    "    for k in range(100):\n",
    "        random_guess_curr = []\n",
    "        for c in range(client_num):\n",
    "            rv = [np.random.random() for _ in range(output_class)]\n",
    "            p = rv / np.sum(rv)\n",
    "            curr_l = []\n",
    "            for l in range(output_class):\n",
    "                curr_l += [l for _ in range(int(2000*p[l]))]\n",
    "            random_guess_curr.append(curr_l)\n",
    "        random_guess.append(random_guess_curr)\n",
    "    return (predit_label, predit_prob, random_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e2c90-02c7-4c2f-8a28-5e93fd2f3332",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LightMLP2\n",
    "#model = LogisticRegression\n",
    "#att_method = 'hc'\n",
    "att_method = 'pgda'\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__\n",
    "    \n",
    "real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, 1)\n",
    "num_feature = global_x.shape[1]\n",
    "\n",
    "att_res = []\n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    kl_sample = []\n",
    "    kl_sample_noise = []\n",
    "    kl_baseline_u = []\n",
    "\n",
    "    for kid in range(1,6):\n",
    "        for _ in range(1,6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "\n",
    "            if att_method == 'hc':\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\")\n",
    "            else:\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\")\n",
    "\n",
    "            #print(sample_list[3].shape)\n",
    "\n",
    "            predit_label, predit_prob, random_guess = naive_label_attack(num_client, sample_list, ml, g)\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for k in range(0,4):\n",
    "                kl_sample.append(wasserstein_distance(predit_label[k], real_y[k]))\n",
    "                kl_sample_noise.append(wasserstein_distance(predit_prob[k], real_y[k]))\n",
    "\n",
    "            # baseline\n",
    "            for k in range(0,4):\n",
    "                for t in range(np.shape(random_guess)[0]):\n",
    "                    kl_baseline_u.append(wasserstein_distance(random_guess[t][k], real_y[k]))\n",
    "\n",
    "    att_res.append(np.mean(kl_sample_noise))\n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(kl_baseline_u))\n",
    "    \n",
    "    print(\"attack g predit: \", np.mean(kl_sample))\n",
    "    print(\"attack pred prob:\", np.mean(kl_sample_noise))\n",
    "    print(\"=======\")\n",
    "print(att_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b630e-8623-4c60-9a42-c7b0cc6c8592",
   "metadata": {},
   "source": [
    "### Varying dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6cd9b-172c-4e83-8bb8-a5492fcfef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_method = 'pgda'\n",
    "\n",
    "\n",
    "att_res = []\n",
    "\n",
    "eps = 4\n",
    "\n",
    "for model in [LightMLP, LightMLP1, LightMLP3, LightMLP4, LightMLP5]:\n",
    "    if hasattr(model, '__name__'):\n",
    "        model_name = model.__name__\n",
    "    \n",
    "    kl_sample = []\n",
    "    kl_sample_noise = []\n",
    "    kl_baseline_u = []\n",
    "\n",
    "    for kid in range(1,6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        \n",
    "        for _ in range(1,6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "\n",
    "            if att_method == 'hc':\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\")\n",
    "            else:\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\")\n",
    "\n",
    "            #print(sample_list[3].shape)\n",
    "\n",
    "            predit_label, predit_prob, random_guess = naive_label_attack(num_client, sample_list, ml, g)\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for k in range(0,4):\n",
    "                kl_sample.append(wasserstein_distance(predit_label[k], real_y[k]))\n",
    "                kl_sample_noise.append(wasserstein_distance(predit_prob[k], real_y[k]))\n",
    "\n",
    "            # baseline\n",
    "            for k in range(0,4):\n",
    "                for t in range(np.shape(random_guess)[0]):\n",
    "                    kl_baseline_u.append(wasserstein_distance(random_guess[t][k], real_y[k]))\n",
    "\n",
    "    att_res.append(np.mean(kl_sample_noise))\n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(kl_baseline_u))\n",
    "    \n",
    "    print(\"attack g predit: \", np.mean(kl_sample))\n",
    "    print(\"attack pred prob:\", np.mean(kl_sample_noise))\n",
    "    print(\"=======\")\n",
    "print(att_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d6932-42a7-43a0-b43f-eb723d9d201b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5ee4c-47ee-41b4-845b-7478246d486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_dpsgd2(eps, i, kid, client_num):\n",
    "    global_iter = 5\n",
    "    w_ml = []\n",
    "        \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/parties='+str(client_num)+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    param_name = list(m0.state_dict().keys())\n",
    "        \n",
    "    mg = model(num_feature, output_class).to(device)\n",
    "    mg.load_state_dict(torch.load('model/'+data_pth+feature+'/parties='+str(client_num)+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    mg.eval()\n",
    "    mg_params = torch.cat([torch.flatten(mg.state_dict()[name]) for name in param_name], -1)\n",
    "            \n",
    "    for client_id in range(client_num):\n",
    "        model_par = []\n",
    "\n",
    "        # compute weights\n",
    "        ml_params = []\n",
    "        ml = []\n",
    "\n",
    "        delta_m_local = []\n",
    "        w_list = []\n",
    "\n",
    "        for epoch in range(1,global_iter+1):\n",
    "            # load local models\n",
    "            m0 = model(num_feature, output_class).to(device)\n",
    "            m0.load_state_dict(torch.load('model/'+data_pth+feature+'/parties='+str(client_num)+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch'+str(epoch)+'/eps'+str(eps)+'/'+str(client_id)+'.pth', \n",
    "                                           map_location=torch.device('cpu')))\n",
    "            m0.eval()\n",
    "            m0_params = torch.cat([torch.flatten(m0.state_dict()[name]) for name in param_name], -1)\n",
    "            ml.append(copy.deepcopy(m0.state_dict()))\n",
    "            \n",
    "            delta_m_local = torch.norm(m0_params - mg_params, p=2)\n",
    "            w_list.append(delta_m_local.item())\n",
    "        w_list = torch.softmax(torch.Tensor(w_list), dim=0)\n",
    "        #print(\"w_list =\", w_list)\n",
    "        new_par = copy.deepcopy(m0.state_dict())\n",
    "        for name in new_par:\n",
    "            new_par[name] = torch.zeros(new_par[name].shape)\n",
    "        for idx, par in enumerate(ml):\n",
    "            for name in new_par:\n",
    "                new_par[name] += par[name]*w_list[idx]\n",
    "                \n",
    "        m0 = model(num_feature, output_class).to(device)\n",
    "        m0.load_state_dict(copy.deepcopy(new_par))\n",
    "        m0.eval()\n",
    "        w_ml.append(copy.deepcopy(m0))\n",
    "\n",
    "    g = model(num_feature, output_class).to(device)\n",
    "    g.load_state_dict(torch.load('model/'+data_pth+feature+'/parties='+str(client_num)+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    g.eval()\n",
    "    \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/parties='+str(client_num)+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    m0.eval()\n",
    "    return (w_ml, g, m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d92b27-1b56-48f0-8990-dc70157a9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "data_pth = 'abalone/'\n",
    "feature = 'length'\n",
    "output_class = 5\n",
    "\n",
    "def load_data2(data_pth, feature, client_num, ki):\n",
    "    # test set\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(1, client_num+1):\n",
    "        x.append(np.load(\"model/\"+data_pth+feature+'/parties='+str(client_num)+\"/c\"+str(ki)+\"/test_x\"+str(i)+\".npy\"))\n",
    "        y.append(np.load(\"model/\"+data_pth+feature+'/parties='+str(client_num)+\"/c\"+str(ki)+\"/test_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    non_iid = []\n",
    "    real_x = []\n",
    "    real_y = []\n",
    "    for i in range(1, client_num+1):\n",
    "        real_x.append(np.load(\"model/\"+data_pth+feature+'/parties='+str(client_num)+\"/c\"+str(ki)+\"/train_x\"+str(i)+\".npy\"))\n",
    "        real_y.append(np.load(\"model/\"+data_pth+feature+'/parties='+str(client_num)+\"/c\"+str(ki)+\"/train_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    global_x = np.concatenate((x[0], x[1]))\n",
    "    global_x = np.concatenate((global_x, x[2]))\n",
    "    global_x = np.concatenate((global_x, x[3]))\n",
    "\n",
    "    global_y = np.concatenate((y[0], y[1]))\n",
    "    global_y = np.concatenate((global_y, y[2]))\n",
    "    global_y = np.concatenate((global_y, y[3]))\n",
    "    #print(global_x.shape)\n",
    "    #print(global_y.shape)\n",
    "    num_feature = global_x.shape[1]\n",
    "    \n",
    "    dist_info = []\n",
    "    discrete_feature = []\n",
    "    for i in range(num_feature):\n",
    "        # discrete?\n",
    "        unique_feature = np.unique(global_x[:, i], return_counts=True)\n",
    "        if len(unique_feature[0]) < 25:\n",
    "            # compute probability (frequence) of each element\n",
    "            dist_info.append(tuple((unique_feature[0], unique_feature[1]/unique_feature[1].sum())))\n",
    "            discrete_feature.append(i)\n",
    "        else:\n",
    "            # compute mean and std\n",
    "            mu = np.mean(global_x[:, i])\n",
    "            sigma = np.std(global_x[:, i])\n",
    "            dist_info.append(tuple((mu, sigma)))\n",
    "        \n",
    "    return real_x, real_y, global_x, global_y, dist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002cdd8-9230-47be-8e3c-b9b951c60f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "# model = LogisticRegression\n",
    "model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "eps = 4\n",
    "for client_num in [6, 8, 10]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = load_data2(data_pth, feature, client_num, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd2(eps, _, kid, client_num)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, client_num, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/p\"+str(client_num)+\"_pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, client_num, fake_sample_size, ml, g, 'uniform')\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(client_num)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(client_num)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,client_num):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a26dd-ae79-4b1d-b19c-4053999b066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "model = LogisticRegression\n",
    "# model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "eps = 4\n",
    "for client_num in [6, 8, 10]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = load_data2(data_pth, feature, client_num, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd2(eps, _, kid, client_num)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, client_num, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/p\"+str(client_num)+\"_pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, client_num, fake_sample_size, ml, g, 'uniform')\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                #np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(client_num)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(client_num)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,client_num):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8004b-27f4-47c7-a418-f3084928647c",
   "metadata": {},
   "source": [
    "### Varying #. Parties -- Label Distribution Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bb5fc-ea39-4e70-9eac-dced6f3cc9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_label_attack(client_num, sample, ml, g):\n",
    "    predit_label = [[] for _ in range(client_num)]\n",
    "    predit_prob = [[] for _ in range(client_num)]\n",
    "    # random_guess = [[np.random.choice(list(range(output_class))) for _ in range(1250)] for _ in range(client_num)]\n",
    "    random_guess = []\n",
    "    for c in range(client_num):\n",
    "        for x in sample[c]:\n",
    "            pred_g = torch.softmax(ml[c](torch.tensor(x).float()),0)\n",
    "                \n",
    "            pred_label = torch.max(pred_g, 0)[1].item()\n",
    "            predit_label[c].append(pred_label)\n",
    "            for i in range(output_class):\n",
    "                for _ in range(100):\n",
    "                    if np.random.random() < pred_g[i]:\n",
    "                        predit_prob[c].append(i)\n",
    "                    \n",
    "        for i in range(output_class):\n",
    "            predit_label[c].append(i)\n",
    "            predit_prob[c].append(i)\n",
    "            \n",
    "    # generate grandom guess results\n",
    "    for k in range(100):\n",
    "        random_guess_curr = []\n",
    "        for c in range(client_num):\n",
    "            rv = [np.random.random() for _ in range(output_class)]\n",
    "            p = rv / np.sum(rv)\n",
    "            curr_l = []\n",
    "            for l in range(output_class):\n",
    "                curr_l += [l for _ in range(int(2000*p[l]))]\n",
    "            random_guess_curr.append(curr_l)\n",
    "        random_guess.append(random_guess_curr)\n",
    "    return (predit_label, predit_prob, random_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81417fa0-206a-4ed0-bf25-ab4428557385",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_method = 'pgda'\n",
    "\n",
    "eps = 4\n",
    "\n",
    "for model in [LightMLP2, LogisticRegression]:\n",
    "    if hasattr(model, '__name__'):\n",
    "        model_name = model.__name__\n",
    "    print(\"++++++ \",model_name ,\" ++++++\" )\n",
    "    \n",
    "    att_res = []\n",
    "    for client_num in [6, 8, 10]:\n",
    "        kl_sample = []\n",
    "        kl_sample_noise = []\n",
    "        kl_baseline_u = []\n",
    "\n",
    "        for kid in range(1,6):\n",
    "            real_x, real_y, global_x, global_y, dist_info = load_data2(data_pth, feature, client_num, kid)\n",
    "            num_feature = global_x.shape[1]\n",
    "            for _ in range(1,6):\n",
    "                ml, g, m0 = load_model_dpsgd2(eps, _, kid, client_num)\n",
    "\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/p\"+str(client_num)+\"_pgd_sample_eps\"+str(eps)+\".npy\")\n",
    "\n",
    "                predit_label, predit_prob, random_guess = naive_label_attack(client_num, sample_list, ml, g)\n",
    "\n",
    "                per_feature = []\n",
    "                per_baseline = []\n",
    "                for k in range(0,client_num):\n",
    "                    kl_sample.append(wasserstein_distance(predit_label[k], real_y[k]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(predit_prob[k], real_y[k]))\n",
    "\n",
    "                # baseline\n",
    "                for k in range(0,client_num):\n",
    "                    for t in range(np.shape(random_guess)[0]):\n",
    "                        kl_baseline_u.append(wasserstein_distance(random_guess[t][k], real_y[k]))\n",
    "\n",
    "        att_res.append(np.mean(kl_sample_noise))\n",
    "        print(\"eps =\", eps)\n",
    "        print(\"baseline uniform: \", np.mean(kl_baseline_u))\n",
    "\n",
    "        print(\"attack g predit: \", np.mean(kl_sample))\n",
    "        print(\"attack pred prob:\", np.mean(kl_sample_noise))\n",
    "        print(\"=======\")\n",
    "    print(att_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb94b5-cda2-403d-ba7a-a34eb6f56b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
