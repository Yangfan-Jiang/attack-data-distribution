{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e309cd-148d-45a9-b14d-0909e6ba7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of FL task\n",
    "from MLModel import *\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm, rv_histogram, entropy, wasserstein_distance\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#export CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807af16d-7f81-4ed8-87d8-64f20e1f1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_client = 4\n",
    "x=[]\n",
    "y=[]\n",
    "data_pth = 'winequality-red/'\n",
    "feature = 'alcohol'\n",
    "\n",
    "output_class = 6\n",
    "\n",
    "def local_data(data_pth, feature, kid):\n",
    "    # test set\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(1, num_client+1):\n",
    "        x.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/test_x\"+str(i)+\".npy\"))\n",
    "        y.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/test_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    non_iid = []\n",
    "    real_x = []\n",
    "    real_y = []\n",
    "    for i in range(1, num_client+1):\n",
    "        real_x.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/train_x\"+str(i)+\".npy\"))\n",
    "        real_y.append(np.load(\"model/\"+data_pth+feature+\"/c\"+str(kid)+\"/train_y\"+str(i)+\".npy\"))\n",
    "\n",
    "    global_x = np.concatenate((x[0], x[1]))\n",
    "    global_x = np.concatenate((global_x, x[2]))\n",
    "    global_x = np.concatenate((global_x, x[3]))\n",
    "\n",
    "    global_y = np.concatenate((y[0], y[1]))\n",
    "    global_y = np.concatenate((global_y, y[2]))\n",
    "    global_y = np.concatenate((global_y, y[3]))\n",
    "    #print(global_x.shape)\n",
    "    #print(global_y.shape)\n",
    "    num_feature = global_x.shape[1]\n",
    "    \n",
    "    dist_info = []\n",
    "    discrete_feature = []\n",
    "    for i in range(num_feature):\n",
    "        # discrete?\n",
    "        unique_feature = np.unique(global_x[:, i], return_counts=True)\n",
    "        if len(unique_feature[0]) < 25:\n",
    "            # compute probability (frequence) of each element\n",
    "            dist_info.append(tuple((unique_feature[0], unique_feature[1]/unique_feature[1].sum())))\n",
    "            discrete_feature.append(i)\n",
    "        else:\n",
    "            # compute mean and std\n",
    "            mu = np.mean(global_x[:, i])\n",
    "            sigma = np.std(global_x[:, i])\n",
    "            dist_info.append(tuple((mu, sigma)))\n",
    "        \n",
    "    return real_x, real_y, global_x, global_y, dist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4819560-ad55-4eaa-99cc-a5010e3d506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_dpsgd(eps, i, kid):\n",
    "    global_iter = 5\n",
    "    w_ml = []\n",
    "        \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    param_name = list(m0.state_dict().keys())\n",
    "        \n",
    "    mg = model(num_feature, output_class).to(device)\n",
    "    mg.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    mg.eval()\n",
    "    mg_params = torch.cat([torch.flatten(mg.state_dict()[name]) for name in param_name], -1)\n",
    "            \n",
    "    for client_id in range(num_client):\n",
    "        model_par = []\n",
    "\n",
    "        # compute weights\n",
    "        ml_params = []\n",
    "        ml = []\n",
    "\n",
    "        delta_m_local = []\n",
    "        w_list = []\n",
    "\n",
    "        for epoch in range(1,global_iter+1):\n",
    "            # load local models\n",
    "            m0 = model(num_feature, output_class).to(device)\n",
    "            m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch'+str(epoch)+'/eps'+str(eps)+'/'+str(client_id)+'.pth', \n",
    "                                           map_location=torch.device('cpu')))\n",
    "            m0.eval()\n",
    "            m0_params = torch.cat([torch.flatten(m0.state_dict()[name]) for name in param_name], -1)\n",
    "            ml.append(copy.deepcopy(m0.state_dict()))\n",
    "            \n",
    "            delta_m_local = torch.norm(m0_params - mg_params, p=2)\n",
    "            w_list.append(delta_m_local.item())\n",
    "            \n",
    "        w_list = torch.softmax(torch.Tensor(w_list), dim=0)\n",
    "\n",
    "        new_par = copy.deepcopy(m0.state_dict())\n",
    "        for name in new_par:\n",
    "            new_par[name] = torch.zeros(new_par[name].shape)\n",
    "        for idx, par in enumerate(ml):\n",
    "            for name in new_par:\n",
    "                new_par[name] += par[name]*w_list[idx]\n",
    "        \n",
    "        m0 = model(num_feature, output_class).to(device)\n",
    "        m0.load_state_dict(copy.deepcopy(new_par))\n",
    "        m0.eval()\n",
    "        w_ml.append(copy.deepcopy(m0))\n",
    "\n",
    "    g = model(num_feature, output_class).to(device)\n",
    "    g.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+'/'+model.__name__+'/epoch5'+'/eps'+str(eps)+'/global.pth', map_location=torch.device('cpu')))\n",
    "    g.eval()\n",
    "    \n",
    "    m0 = model(num_feature, output_class).to(device)\n",
    "    m0.load_state_dict(torch.load('model/'+data_pth+feature+'/c'+str(kid)+\"/\"+str(i)+\"/\"+model.__name__+'/init/eps'+str(eps)+'/init.pth', map_location=torch.device('cpu')))\n",
    "    m0.eval()\n",
    "    return (w_ml, g, m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87510c32-bdf6-48e2-b5ca-6b60a08977b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack(c, feature_dis, num_feature, ml, g, lr, var_factor):\n",
    "    r_min = 0\n",
    "    r_max = 1\n",
    "    x = torch.tensor(np.random.uniform(r_min, r_max, num_feature)).float().requires_grad_()\n",
    "    opt = optim.Adam([x], lr=lr)\n",
    "\n",
    "    for _ in range(25):\n",
    "        yg = torch.softmax(g(x),0)\n",
    "        yc = torch.softmax(ml[c](x),0)\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(yg, yc) + var_factor*x.var()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # projection\n",
    "        x = torch.clamp(x, r_min, r_max).clone().detach().numpy()\n",
    "        for f in range(num_feature):\n",
    "            # discrete data\n",
    "            if type(feature_dis[f][0]) is np.ndarray:\n",
    "                x[f] = np.random.choice(feature_dis[f][0], 1)[0]\n",
    "        x = torch.tensor(x).requires_grad_()\n",
    "        opt = optim.Adam([x], lr=lr)\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def gen_synthesis_PGD(dist_info, num_feature, num_client, fake_sample_size, ml, g, lr, var_factor):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    for c in range(num_client):\n",
    "        tmpsample = []\n",
    "        for i in range(fake_sample_size):\n",
    "            x = PGD_attack(c, dist_info, num_feature, ml, g, lr, var_factor)\n",
    "            tmpsample.append(x)\n",
    "            if len(tmpsample)%400 == 0:\n",
    "                print(len(tmpsample))\n",
    "        sample_list.append(copy.deepcopy(np.array(tmpsample)))\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def PGD_noised_sample(feature_dis, num_feature, num_client, fake_sample_size, ml, g, m0, lr, var_factor):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    sample_sim_list = []\n",
    "    sample_noise_lsit = []\n",
    "    sample_simnoise_lise = []\n",
    "    subsample_list = []\n",
    "    \n",
    "    for c in range(num_client):\n",
    "        pgd_sample = []\n",
    "        pgd_noise = []\n",
    "\n",
    "        d_max = -1\n",
    "        d_min = 9999\n",
    "\n",
    "        cnt = 0\n",
    "        d_list = []\n",
    "        d_threshold  = 0\n",
    "        \n",
    "        while len(pgd_noise) < fake_sample_size:\n",
    "            x = PGD_attack(c, feature_dis, num_feature, ml, g, lr, var_factor)\n",
    "            if len(pgd_sample) < fake_sample_size:\n",
    "                pgd_sample.append(copy.deepcopy(x))\n",
    "                pgd_noise.append(copy.deepcopy(x))\n",
    "\n",
    "        sample_list.append(copy.deepcopy(np.array(pgd_sample)))\n",
    "        sample_noise_lsit.append(copy.deepcopy(np.array(pgd_noise)))\n",
    "    return (sample_list, sample_noise_lsit)\n",
    "\n",
    "\n",
    "\n",
    "def hc_noise_attack(feature_dis, num_feature, num_client, fake_sample_size, ml, g, prior_dist):\n",
    "    fake_sample_size = int(fake_sample_size / num_client)\n",
    "    sample_list = []\n",
    "    subsample_list = []\n",
    "    \n",
    "    for c in range(num_client):\n",
    "        hc_sample = []\n",
    "        hc_subsample = []\n",
    "        threshold = 1e-3\n",
    "        \n",
    "        d_max = -1\n",
    "        d_min = 9999\n",
    "        \n",
    "        cnt = 0\n",
    "        d_list = []\n",
    "        d_threshold  = 0\n",
    "        while len(hc_subsample) < fake_sample_size:\n",
    "            x = hc_attack(c, feature_dis, num_feature, ml, g, prior_dist, threshold)\n",
    "            j = 0\n",
    "            \n",
    "            # generate a valid sample\n",
    "            while x is None:\n",
    "                x = hc_attack(c, feature_dis, num_feature, ml, g, prior_dist, threshold)\n",
    "                j += 1\n",
    "                if j > 10:\n",
    "                    threshold *= 2\n",
    "                    #print(\"threshold =\", threshold)\n",
    "                    j = 0\n",
    "                    \n",
    "            if len(hc_sample) < fake_sample_size:\n",
    "                hc_sample.append(copy.deepcopy(x))\n",
    "                hc_subsample.append(copy.deepcopy(x))\n",
    "            \n",
    "        sample_list.append(copy.deepcopy(np.array(hc_sample)))\n",
    "        subsample_list.append(copy.deepcopy(np.array(hc_subsample)))\n",
    "    return (sample_list, subsample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3decca-c39a-483f-8a22-e8cb4d494095",
   "metadata": {},
   "source": [
    "## Attack LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b49bd7-ea9f-4a50-864d-6a178a5b9e11",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "model = LogisticRegression\n",
    "# model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e6333-39cd-40e5-9940-edf957486e46",
   "metadata": {},
   "source": [
    "## Black-Box LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12bd81-7389-4241-b6f0-35fc420e9b07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'hc'\n",
    "model = LogisticRegression\n",
    "# model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f21f2-7c0a-47c7-bc0a-82a9cbc7cade",
   "metadata": {},
   "source": [
    "## PGDA NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3466a-b562-4764-880c-0275468fb06e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "att_method = 'pgd'\n",
    "# model = LogisticRegression\n",
    "model = LightMLP2\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__ \n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    rg_uni = []\n",
    "    rg_gua = []\n",
    "    \n",
    "    pgd_att = []\n",
    "    # hc_att = []\n",
    "    \n",
    "    for kid in range(1, 6):\n",
    "        real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, kid)\n",
    "        num_feature = global_x.shape[1]\n",
    "        for _ in range(1, 6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "            fake_sample_size = 4000\n",
    "\n",
    "            if att_method == 'pgd':\n",
    "                sample_list, pgd_noise = PGD_noised_sample(dist_info, num_feature, num_client, fake_sample_size, ml, g, m0, 0.1, 0.1)\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "                \n",
    "            elif att_method == 'hc':\n",
    "                sample_list, pgd_noise = hc_noise_attack(dist_info, num_feature, num_client, fake_sample_size, ml, g, 'uniform')\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\", np.array(sample_list))\n",
    "                np.save('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_noise_sample_eps\"+str(eps)+\".npy\", np.array(pgd_noise))\n",
    "\n",
    "            r_min = 0\n",
    "            r_max = 1\n",
    "\n",
    "            gaussian_est = [np.array(synthetic_gaussian(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "            uniform_est = [np.array(synthetic_uniform(dist_info, num_feature, int(fake_sample_size/4))) for _ in range(4)]\n",
    "\n",
    "            kl_baseline_g = []\n",
    "            kl_baseline_u = []\n",
    "\n",
    "            kl_sample = []\n",
    "            kl_sample_noise = []\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for n_feature in range(num_feature):\n",
    "                for k in range(0,4):\n",
    "                    kl_baseline_g.append(wasserstein_distance(gaussian_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_baseline_u.append(wasserstein_distance(uniform_est[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "\n",
    "                    kl_sample.append(wasserstein_distance(sample_list[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                    kl_sample_noise.append(wasserstein_distance(pgd_noise[k][:,n_feature], real_x[k][:,n_feature]))\n",
    "                per_feature.append(np.mean(kl_sample[-4:]))\n",
    "                \n",
    "            rg_uni.append(np.mean(kl_baseline_u))\n",
    "            rg_gua.append(np.mean(kl_baseline_g))\n",
    "            pgd_att.append(np.mean(kl_sample))\n",
    "    \n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(rg_uni))\n",
    "    print(\"baseline gaussian:\", np.mean(rg_gua))\n",
    "    \n",
    "    print(\"attack: \", np.mean(pgd_att))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4ca97-8ddf-414e-9ece-c99411a94dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5fc467-7d4c-494a-9fb2-d464abcf335d",
   "metadata": {},
   "source": [
    "## Attack Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000aedc-f080-417f-a642-14fbec676b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_label_attack(client_num, sample, ml, g):\n",
    "    predit_label = [[] for _ in range(client_num)]\n",
    "    predit_prob = [[] for _ in range(client_num)]\n",
    "    # random_guess = [[np.random.choice(list(range(output_class))) for _ in range(1250)] for _ in range(client_num)]\n",
    "    random_guess = []\n",
    "    for c in range(client_num):\n",
    "        for x in sample[c]:\n",
    "            pred_g = torch.softmax(ml[c](torch.tensor(x).float()),0)\n",
    "                \n",
    "            pred_label = torch.max(pred_g, 0)[1].item()\n",
    "            predit_label[c].append(pred_label)\n",
    "            for i in range(output_class):\n",
    "                for _ in range(100):\n",
    "                    if np.random.random() < pred_g[i]:\n",
    "                        predit_prob[c].append(i)\n",
    "                    \n",
    "        for i in range(output_class):\n",
    "            predit_label[c].append(i)\n",
    "            predit_prob[c].append(i)\n",
    "            \n",
    "    # generate grandom guess results\n",
    "    for k in range(100):\n",
    "        random_guess_curr = []\n",
    "        for c in range(client_num):\n",
    "            rv = [np.random.random() for _ in range(output_class)]\n",
    "            p = rv / np.sum(rv)\n",
    "            curr_l = []\n",
    "            for l in range(output_class):\n",
    "                curr_l += [l for _ in range(int(2000*p[l]))]\n",
    "            random_guess_curr.append(curr_l)\n",
    "        random_guess.append(random_guess_curr)\n",
    "    return (predit_label, predit_prob, random_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820bf3-b531-440e-b7fa-3c27590cafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightMLP2\n",
    "#model = LogisticRegression\n",
    "#att_method = 'hc'\n",
    "att_method = 'pgda'\n",
    "\n",
    "if hasattr(model, '__name__'):\n",
    "    model_name = model.__name__\n",
    "    \n",
    "real_x, real_y, global_x, global_y, dist_info = local_data(data_pth, feature, 1)\n",
    "num_feature = global_x.shape[1]\n",
    "\n",
    "att_res = []\n",
    "\n",
    "for eps in [0.25, 0.5, 1, 2, 4, 8, 1e12]:\n",
    "    kl_sample = []\n",
    "    kl_sample_noise = []\n",
    "    kl_baseline_u = []\n",
    "\n",
    "    for kid in range(1,6):\n",
    "        for _ in range(1,6):\n",
    "            ml, g, m0 = load_model_dpsgd(eps, _, kid)\n",
    "\n",
    "            if att_method == 'hc':\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/hc_sample_eps\"+str(eps)+\".npy\")\n",
    "            else:\n",
    "                sample_list = np.load('model/'+data_pth+feature+'/c'+str(kid)+'/'+str(_)+'/'+model_name+\"/pgd_sample_eps\"+str(eps)+\".npy\")\n",
    "\n",
    "            #print(sample_list[3].shape)\n",
    "\n",
    "            predit_label, predit_prob, random_guess = naive_label_attack(num_client, sample_list, ml, g)\n",
    "\n",
    "            per_feature = []\n",
    "            per_baseline = []\n",
    "            for k in range(0,4):\n",
    "                kl_sample.append(wasserstein_distance(predit_label[k], real_y[k]))\n",
    "                kl_sample_noise.append(wasserstein_distance(predit_prob[k], real_y[k]))\n",
    "\n",
    "            # baseline\n",
    "            for k in range(0,4):\n",
    "                for t in range(np.shape(random_guess)[0]):\n",
    "                    kl_baseline_u.append(wasserstein_distance(random_guess[t][k], real_y[k]))\n",
    "\n",
    "    att_res.append(np.mean(kl_sample_noise))\n",
    "    print(\"eps =\", eps)\n",
    "    print(\"baseline uniform: \", np.mean(kl_baseline_u))\n",
    "    \n",
    "    print(\"attack g predit: \", np.mean(kl_sample))\n",
    "    print(\"attack pred prob:\", np.mean(kl_sample_noise))\n",
    "    print(\"=======\")\n",
    "print(att_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c4252-ed1b-4348-abfc-83574298f4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
